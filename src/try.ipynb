{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "import calc as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = \"../data/tagged.json\"\n",
    "\n",
    "# Read words from the input JSON file\n",
    "with open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as infile:\n",
    "    base_vocab = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the 'word2vec-google-news-300' model...\n",
      "Model loaded successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SemCalculator\n",
    "sem_calc = c.SemCalculator(base_vocab=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'man - (king - queen)':\n",
      "woman: 0.7187\n",
      "man: 0.6558\n",
      "girl: 0.5883\n",
      "lady: 0.5754\n",
      "teenager: 0.5378\n",
      "schoolgirl: 0.4978\n",
      "policewoman: 0.4907\n",
      "blonde: 0.4871\n",
      "redhead: 0.4778\n",
      "brunette: 0.4762\n"
     ]
    }
   ],
   "source": [
    "# Compute difference vector: king - queen\n",
    "difference_vec = sem_calc.subtract_vectors('king', 'queen')\n",
    "\n",
    "# Add the difference vector to 'man'\n",
    "new_vec = sem_calc.subtract_vectors('man', difference_vec)\n",
    "\n",
    "# Find the most similar words to the new vector\n",
    "similar_words = sem_calc.get_most_similar_words(new_vec, topn=10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Most similar words to 'man - (king - queen)':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('recurse', 1.0000001192092896), ('delimiter', 0.5342383980751038), ('cardinality', 0.5059214234352112), ('tuple', 0.49939844012260437), ('obj', 0.4907994270324707), ('pathname', 0.488739013671875), ('preprocessor', 0.48395562171936035)]\n"
     ]
    }
   ],
   "source": [
    "print(sem_calc.get_most_similar_words(\"recurse\", topn=20, pos_filter=\"NN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrelated_words = sem_calc.get_most_unrelated_words([\"test\", \"starcraft\"], pos_filter=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('economist', 1.1239797621965408), ('poignancy', 1.1229719780385494), ('stridency', 1.1031844671815634), ('preoccupation', 1.0996383093297482), ('gratitude', 1.099144585430622), ('shopkeeper', 1.098434541374445), ('ebullience', 1.0983146633952856), ('dais', 1.0931188948452473), ('shrillness', 1.0919341165572405), ('parishioner', 1.0871334224939346)]\n"
     ]
    }
   ],
   "source": [
    "print(unrelated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maximally_unrelated_words(sem_calc, k, pos_filter=None):\n",
    "    \"\"\"\n",
    "    Find a set of k words that are maximally semantically unrelated.\n",
    "\n",
    "    Parameters:\n",
    "    - sem_calc: An instance of SemCalculator.\n",
    "    - k: The number of words to find.\n",
    "    - pos_filter: List of POS tags to include (e.g., ['NN'] for nouns).\n",
    "\n",
    "    Returns:\n",
    "    - A list of words that are maximally semantically unrelated.\n",
    "    \"\"\"\n",
    "    # Initialize the set with a random word from the vocabulary\n",
    "    available_words = list(sem_calc.vocab)\n",
    "    if pos_filter:\n",
    "        # Filter available words by POS tags\n",
    "        available_words = [\n",
    "            word for word in available_words\n",
    "            if sem_calc.word_pos_tags.get(word, \"\") in pos_filter and word in sem_calc.model\n",
    "        ]\n",
    "    else:\n",
    "        # Ensure words are in the model's vocabulary\n",
    "        available_words = [word for word in available_words if word in sem_calc.model]\n",
    "\n",
    "    if len(available_words) < k:\n",
    "        raise ValueError(f\"Not enough words available to find {k} unrelated words.\")\n",
    "\n",
    "    # Randomly select the first word\n",
    "    first_word = random.choice(available_words)\n",
    "    selected_words = [first_word]\n",
    "    available_words.remove(first_word)\n",
    "\n",
    "    print(f\"Starting word: {first_word}\")\n",
    "\n",
    "    # Iteratively select the most unrelated word\n",
    "    while len(selected_words) < k:\n",
    "        # Find the most unrelated words to the current set\n",
    "        unrelated_words = sem_calc.get_most_unrelated_words(\n",
    "            input_words_or_vectors=selected_words,\n",
    "            topn=100,  # Get more candidates to avoid already selected words\n",
    "            pos_filter=pos_filter\n",
    "        )\n",
    "\n",
    "        # Filter out words already selected or not available\n",
    "        for word, _ in unrelated_words:\n",
    "            if word not in selected_words and word in available_words:\n",
    "                selected_words.append(word)\n",
    "                available_words.remove(word)\n",
    "                print(f\"Selected word {len(selected_words)}: {word}\")\n",
    "                break\n",
    "        else:\n",
    "            # If no new word is found, break the loop\n",
    "            print(\"No more unrelated words can be found.\")\n",
    "            break\n",
    "\n",
    "    return selected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting word: hydrogenation\n"
     ]
    }
   ],
   "source": [
    "# Find a set of 5 maximally semantically unrelated words among nouns\n",
    "k = 1\n",
    "pos_filter = ['NN']  # Only consider nouns\n",
    "unrelated_words_set = find_maximally_unrelated_words(sem_calc, k, pos_filter=pos_filter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bighead',\n",
       " 'sonneratia',\n",
       " 'scaffie',\n",
       " 'plerophory',\n",
       " 'dotage',\n",
       " 'barometer',\n",
       " 'atloidoaxoid',\n",
       " 'campanile',\n",
       " 'trabuco',\n",
       " 'trophaea',\n",
       " 'lapboard',\n",
       " 'archbishop',\n",
       " 'flashbulbs',\n",
       " 'trustier',\n",
       " 'astarboard',\n",
       " 'shadowbox',\n",
       " 'splenius',\n",
       " 'duettino',\n",
       " 'odophone',\n",
       " 'beggarhood']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sampling nouns\n",
    "nouns = random.sample([k for k,v in base_vocab.items() if v==\"NN\"], 20)\n",
    "\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximally unrelated words starting from 'quality':\n",
      "1: quality\n",
      "2: initialism\n",
      "3: havocs\n",
      "4: dragster\n",
      "5: pedometer\n",
      "6: month\n"
     ]
    }
   ],
   "source": [
    "k_more = 0\n",
    "words = [\"quality\", \"initialism\", \"havocs\", \"dragster\", \"pedometer\", \"month\"]\n",
    "\n",
    "for i in range(k_more):\n",
    "    # Get the most unrelated word to the current list of words\n",
    "    # It returns a list of tuples [(word, distance)], so we extract the word\n",
    "    word, distance = sem_calc.get_most_unrelated_words(words, 1, pos_filter=['NN'])[0]\n",
    "    words.append(word)\n",
    "\n",
    "# Print the resulting words\n",
    "print(\"Maximally unrelated words starting from 'quality':\")\n",
    "for idx, word in enumerate(words, 1):\n",
    "    print(f\"{idx}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 30071 words into 9 clusters...\n",
      "Cluster 1: Top 10 words: tchr, bibliog, quate, fringilla, chiasm, latticelike, hyperpyrexia, plexiform, centage, gametophyte\n",
      "Cluster 2: Top 10 words: potoroo, yellowhead, sengi, bladderwort, clethra, arrowwood, stringybark, cistus, jerboa, elodea\n",
      "Cluster 3: Top 10 words: meshuga, puddy, caramba, moper, gagster, ringy, kewpie, snaggle, hoppity, booky\n",
      "Cluster 4: Top 10 words: derange, anthropomorphise, preordain, unfetter, pussyfoot, unhorse, dissatisfy, importune, feminise, wisen\n",
      "Cluster 5: Top 10 words: almondy, vacherin, farfel, verjuice, paprikas, demerara, orgeat, bavarois, confiture, souffl\n",
      "Cluster 6: Top 10 words: mortice, dacron, turtleback, snowlike, vigas, batiste, headpin, karabiner, reflexed, watchcase\n",
      "Cluster 7: Top 10 words: tortor, dawk, risus, viverra, traduction, dilo, naf, saul, irfan, donnie\n",
      "Cluster 8: Top 10 words: eccrine, leiomyoma, degranulation, cytidine, isoenzyme, sitosterol, apocrine, hyperkeratosis, hydroperoxide, leiomyomas\n",
      "Cluster 9: Top 10 words: incommensurability, referentiality, ressentiment, sensibleness, indifferentism, reactionism, alterity, bewitchment, nowness, animality\n",
      "\n",
      "Selected words:\n",
      "1: 0\n",
      "2: 1\n",
      "3: 2\n",
      "4: 3\n",
      "5: 4\n",
      "6: 5\n",
      "7: 6\n",
      "8: 7\n",
      "9: 8\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "pos_tags = ['NN']          # POS tags to include\n",
    "num_clusters = 9          # Number of clusters / words to select\n",
    "\n",
    "# Find diverse words (works worse than greedy)\n",
    "diverse_words = sem_calc.find_top_words_by_clustering(pos_tags, num_clusters)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nSelected words:\")\n",
    "for idx, word in enumerate(diverse_words, 1):\n",
    "    print(f\"{idx}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'apple' and 'orange': 0.3920\n",
      "\n",
      "Shared connection between 'cat', 'dog', 'rabbit':\n",
      "cat: 0.9105\n",
      "dog: 0.8947\n",
      "rabbit: 0.8304\n",
      "puppy: 0.7766\n",
      "beagle: 0.7664\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity between 'apple' and 'orange'\n",
    "similarity = sem_calc.compute_similarity('apple', 'orange')\n",
    "print(f\"Similarity between 'apple' and 'orange': {similarity:.4f}\")\n",
    "\n",
    "# Find shared connection between 'cat', 'dog', 'rabbit'\n",
    "shared_connection = sem_calc.find_shared_connection(['cat', 'dog', 'rabbit'])\n",
    "print(\"\\nShared connection between 'cat', 'dog', 'rabbit':\")\n",
    "for word, score in shared_connection:\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
