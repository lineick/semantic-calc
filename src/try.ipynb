{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "import calc as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = \"../data/words_dat.json\"\n",
    "\n",
    "# Read words from the input JSON file\n",
    "with open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as infile:\n",
    "    base_vocab = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the 'word2vec-google-news-300' model...\n",
      "Model loaded successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SemCalculator\n",
    "sem_calc = c.SemCalculator(base_vocab=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'man - (king - queen)':\n",
      "woman: 0.7187\n",
      "man: 0.6558\n",
      "girl: 0.5883\n",
      "lady: 0.5754\n",
      "teenager: 0.5378\n",
      "schoolgirl: 0.4978\n",
      "policewoman: 0.4907\n",
      "blonde: 0.4871\n",
      "redhead: 0.4778\n",
      "brunette: 0.4762\n"
     ]
    }
   ],
   "source": [
    "# Compute difference vector: king - queen\n",
    "difference_vec = sem_calc.subtract_vectors('king', 'queen')\n",
    "\n",
    "# Add the difference vector to 'man'\n",
    "new_vec = sem_calc.subtract_vectors('man', difference_vec)\n",
    "\n",
    "# Find the most similar words to the new vector\n",
    "similar_words = sem_calc.get_most_similar_words(new_vec, topn=10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Most similar words to 'man - (king - queen)':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('car', 1.0000001192092896), ('vehicle', 0.7821096181869507), ('cars', 0.7423831224441528), ('minivan', 0.6907036900520325), ('truck', 0.6735789775848389), ('scooter', 0.6381530165672302), ('sedan', 0.6336701512336731), ('motorcycle', 0.6256055235862732), ('van', 0.6115673780441284), ('vehicles', 0.5998871326446533), ('motorbike', 0.5921168923377991), ('bike', 0.5854154229164124), ('automobile', 0.5838367342948914), ('driver', 0.577939510345459)]\n"
     ]
    }
   ],
   "source": [
    "print(sem_calc.get_most_similar_words(\"car\", topn=20, pos_filter=\"NN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrelated_words = sem_calc.get_most_unrelated_words([\"test\", \"starcraft\"], pos_filter=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('economist', 1.1239797621965408), ('poignancy', 1.1229719780385494), ('staples', 1.114982157945633), ('segued', 1.1101001650094986), ('tinware', 1.1093841940164566), ('directness', 1.1070785825140774), ('pained', 1.1067325808107853), ('somberness', 1.1062901727855206), ('macroeconomist', 1.1035850867629051), ('stridency', 1.1031844671815634)]\n"
     ]
    }
   ],
   "source": [
    "print(unrelated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maximally_unrelated_words(sem_calc, k, pos_filter=None):\n",
    "    \"\"\"\n",
    "    Find a set of k words that are maximally semantically unrelated.\n",
    "\n",
    "    Parameters:\n",
    "    - sem_calc: An instance of SemCalculator.\n",
    "    - k: The number of words to find.\n",
    "    - pos_filter: List of POS tags to include (e.g., ['NN'] for nouns).\n",
    "\n",
    "    Returns:\n",
    "    - A list of words that are maximally semantically unrelated.\n",
    "    \"\"\"\n",
    "    # Initialize the set with a random word from the vocabulary\n",
    "    available_words = list(sem_calc.vocab)\n",
    "    if pos_filter:\n",
    "        # Filter available words by POS tags\n",
    "        available_words = [\n",
    "            word for word in available_words\n",
    "            if sem_calc.word_pos_tags.get(word, \"\") in pos_filter and word in sem_calc.model\n",
    "        ]\n",
    "    else:\n",
    "        # Ensure words are in the model's vocabulary\n",
    "        available_words = [word for word in available_words if word in sem_calc.model]\n",
    "\n",
    "    if len(available_words) < k:\n",
    "        raise ValueError(f\"Not enough words available to find {k} unrelated words.\")\n",
    "\n",
    "    # Randomly select the first word\n",
    "    first_word = random.choice(available_words)\n",
    "    selected_words = [first_word]\n",
    "    available_words.remove(first_word)\n",
    "\n",
    "    print(f\"Starting word: {first_word}\")\n",
    "\n",
    "    # Iteratively select the most unrelated word\n",
    "    while len(selected_words) < k:\n",
    "        # Find the most unrelated words to the current set\n",
    "        unrelated_words = sem_calc.get_most_unrelated_words(\n",
    "            input_words_or_vectors=selected_words,\n",
    "            topn=100,  # Get more candidates to avoid already selected words\n",
    "            pos_filter=pos_filter\n",
    "        )\n",
    "\n",
    "        # Filter out words already selected or not available\n",
    "        for word, _ in unrelated_words:\n",
    "            if word not in selected_words and word in available_words:\n",
    "                selected_words.append(word)\n",
    "                available_words.remove(word)\n",
    "                print(f\"Selected word {len(selected_words)}: {word}\")\n",
    "                break\n",
    "        else:\n",
    "            # If no new word is found, break the loop\n",
    "            print(\"No more unrelated words can be found.\")\n",
    "            break\n",
    "\n",
    "    return selected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting word: mavericks\n",
      "Selected word 2: curability\n",
      "Selected word 3: finalizing\n",
      "Selected word 4: conner\n",
      "Selected word 5: inventoried\n",
      "Selected word 6: gained\n",
      "Selected word 7: charades\n"
     ]
    }
   ],
   "source": [
    "# Find a set of 5 maximally semantically unrelated words among nouns\n",
    "k = 7\n",
    "pos_filter = ['NN']  # Only consider nouns\n",
    "unrelated_words_set = find_maximally_unrelated_words(sem_calc, k, pos_filter=pos_filter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crotonic',\n",
       " 'refractometriably',\n",
       " 'donkey',\n",
       " 'diagnostics',\n",
       " 'winkles',\n",
       " 'xerographic',\n",
       " 'mispromethazinisability']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sampling nouns\n",
    "nouns = random.sample([k for k,v in base_vocab.items() if v==\"NN\"], 7)\n",
    "\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximally unrelated words starting from 'quality':\n",
      "1: havoc\n",
      "2: thermography\n",
      "3: pledges\n",
      "4: sibling\n",
      "5: quarrier\n",
      "6: subheading\n",
      "7: timeliest\n",
      "8: acquit\n",
      "9: commissary\n",
      "10: attempting\n"
     ]
    }
   ],
   "source": [
    "k_more = 9\n",
    "words = [\"havoc\"]\n",
    "\n",
    "for i in range(k_more):\n",
    "    # Get the most unrelated word to the current list of words\n",
    "    # It returns a list of tuples [(word, distance)], so we extract the word\n",
    "    word, distance = sem_calc.get_most_unrelated_words(words, 1, pos_filter=['NN'])[0]\n",
    "    words.append(word)\n",
    "\n",
    "# Print the resulting words\n",
    "print(\"Maximally unrelated words starting from 'quality':\")\n",
    "for idx, word in enumerate(words, 1):\n",
    "    print(f\"{idx}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 87874 words into 7 clusters...\n",
      "Cluster 1: Top 10 words: pyrimidines, trophoblasts, cytidine, isoprenoid, metaplasia, degranulation, hexose, hydroperoxide, oxidases, leiomyoma\n",
      "Cluster 2: Top 10 words: jubbly, caramba, hipsterish, wakey, swart, dillies, shitbag, pouters, staidly, plonky\n",
      "Cluster 3: Top 10 words: habitude, ambiguousness, unconfessed, denotative, referentiality, ascriptions, incommensurability, indifferentism, abjectness, attitudinizing\n",
      "Cluster 4: Top 10 words: dumdum, sallying, revilers, pussyfooted, roached, frogged, rived, vitalised, raying, ladino\n",
      "Cluster 5: Top 10 words: irrebuttable, spinous, unpaged, paribus, polyphenylene, gravis, rheumatica, corpuscular, succinic, neurofibrillary\n",
      "Cluster 6: Top 10 words: unhinges, overextends, atomizes, flusters, comes, expunges, verbalizes, goes, engrosses, emasculates\n",
      "Cluster 7: Top 10 words: yellowthroat, glacÃ©, gorgonians, ripply, stockinette, darner, turnstone, jardinieres, buttercreams, lavatera\n",
      "\n",
      "Selected words:\n",
      "1: 0\n",
      "2: 1\n",
      "3: 2\n",
      "4: 3\n",
      "5: 4\n",
      "6: 5\n",
      "7: 6\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "pos_tags = ['NN']          # POS tags to include\n",
    "num_clusters = 7          # Number of clusters / words to select\n",
    "\n",
    "# Find diverse words (works worse than greedy)\n",
    "diverse_words = sem_calc.find_top_words_by_clustering(pos_tags, num_clusters)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nSelected words:\")\n",
    "for idx, word in enumerate(diverse_words, 1):\n",
    "    print(f\"{idx}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'apple' and 'orange': 0.3920\n",
      "\n",
      "Shared connection between 'cat', 'dog', 'rabbit':\n",
      "cat: 0.9105\n",
      "dog: 0.8947\n",
      "rabbit: 0.8304\n",
      "puppy: 0.7766\n",
      "beagle: 0.7664\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity between 'apple' and 'orange'\n",
    "similarity = sem_calc.compute_similarity('apple', 'orange')\n",
    "print(f\"Similarity between 'apple' and 'orange': {similarity:.4f}\")\n",
    "\n",
    "# Find shared connection between 'cat', 'dog', 'rabbit'\n",
    "shared_connection = sem_calc.find_shared_connection(['cat', 'dog', 'rabbit'])\n",
    "print(\"\\nShared connection between 'cat', 'dog', 'rabbit':\")\n",
    "for word, score in shared_connection:\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
