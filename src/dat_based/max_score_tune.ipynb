{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "import scipy.spatial.distance\n",
    "\n",
    "def load_model_vectors(model_path, words_set):\n",
    "    vectors = {}\n",
    "    with open(model_path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.rstrip().split(\" \")\n",
    "            word = tokens[0].lower()\n",
    "            if word in words_set:\n",
    "                vector = np.asarray(tokens[1:], dtype=\"float32\")\n",
    "                vectors[word] = vector\n",
    "    return vectors\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, model=\"glove.840B.300d.txt\", dictionary=\"words.txt\", pattern=\"^[a-z][a-z-]*[a-z]$\"):\n",
    "        words_set = set()\n",
    "        with open(dictionary, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if re.match(pattern, line):\n",
    "                    words_set.add(line.lower())\n",
    "\n",
    "        self.vectors_dict = load_model_vectors(model, words_set)\n",
    "        self.words_array = np.array(list(self.vectors_dict.keys()))\n",
    "        self.word_vectors = np.array(list(self.vectors_dict.values()))\n",
    "        self.vectors = self.vectors_dict  # For compatibility\n",
    "\n",
    "    def validate(self, word):\n",
    "        \"\"\"Clean up word and find best candidate to use\"\"\"\n",
    "        # Strip unwanted characters\n",
    "        clean = re.sub(r\"[^a-zA-Z- ]+\", \"\", word).strip().lower()\n",
    "        if len(clean) <= 1:\n",
    "            return None  # Word too short\n",
    "\n",
    "        # Generate candidates for possible compound words\n",
    "        candidates = []\n",
    "        if \" \" in clean:\n",
    "            candidates.append(re.sub(r\" +\", \"-\", clean))\n",
    "            candidates.append(re.sub(r\" +\", \"\", clean))\n",
    "        else:\n",
    "            candidates.append(clean)\n",
    "            if \"-\" in clean:\n",
    "                candidates.append(re.sub(r\"-+\", \"\", clean))\n",
    "        for cand in candidates:\n",
    "            if cand in self.vectors:\n",
    "                return cand  # Return first word that is in model\n",
    "        return None  # Could not find valid word\n",
    "\n",
    "    def distance(self, word1, word2):\n",
    "        \"\"\"Compute cosine distance (0 to 2) between two words\"\"\"\n",
    "        vec1 = self.vectors.get(word1)\n",
    "        vec2 = self.vectors.get(word2)\n",
    "        if vec1 is not None and vec2 is not None:\n",
    "            return scipy.spatial.distance.cosine(vec1, vec2)\n",
    "        else:\n",
    "            return 0.0  # Or handle as appropriate\n",
    "\n",
    "    def dat(self, words, minimum=7):\n",
    "        \"\"\"Compute DAT score\"\"\"\n",
    "        # Keep only valid unique words\n",
    "        uniques = []\n",
    "        for word in words:\n",
    "            valid = self.validate(word)\n",
    "            if valid and valid not in uniques:\n",
    "                uniques.append(valid)\n",
    "\n",
    "        # Keep subset of words\n",
    "        if len(uniques) >= minimum:\n",
    "            subset = uniques[:minimum]\n",
    "        else:\n",
    "            return None  # Not enough valid words\n",
    "\n",
    "        # Compute distances between each pair of words\n",
    "        distances = []\n",
    "        for word1, word2 in itertools.combinations(subset, 2):\n",
    "            dist = self.distance(word1, word2)\n",
    "            distances.append(dist)\n",
    "\n",
    "        # Compute the DAT score (average semantic distance multiplied by 100)\n",
    "        return (sum(distances) / len(distances)) * 100\n",
    "\n",
    "def find_max_dissimilar_words(model, k=7, num_iterations=5):\n",
    "    # Initial greedy selection\n",
    "    words = model.words_array\n",
    "    word_vectors = model.word_vectors\n",
    "    vectors = model.vectors\n",
    "\n",
    "    selected_indices = []\n",
    "    remaining_indices = np.arange(len(words))\n",
    "\n",
    "    # Randomly select the first word\n",
    "    first_idx = np.random.choice(remaining_indices)\n",
    "    selected_indices.append(first_idx)\n",
    "    remaining_indices = np.delete(remaining_indices, np.where(remaining_indices == first_idx))\n",
    "\n",
    "    selected_vecs = [word_vectors[first_idx]]\n",
    "\n",
    "    for _ in range(k - 1):\n",
    "        # Compute distances from selected words to all remaining words\n",
    "        dists = scipy.spatial.distance.cdist(word_vectors[remaining_indices], np.vstack(selected_vecs), 'cosine')\n",
    "        min_dists = np.min(dists, axis=1)\n",
    "\n",
    "        # Select the word with the maximum of these minimum distances\n",
    "        max_min_dist_idx = np.argmax(min_dists)\n",
    "        next_idx = remaining_indices[max_min_dist_idx]\n",
    "        selected_indices.append(next_idx)\n",
    "        selected_vecs.append(word_vectors[next_idx])\n",
    "\n",
    "        # Remove selected index from remaining\n",
    "        remaining_indices = np.delete(remaining_indices, max_min_dist_idx)\n",
    "\n",
    "    selected_words = [words[idx] for idx in selected_indices]\n",
    "    best_score = model.dat(selected_words)\n",
    "    best_words = selected_words.copy()\n",
    "\n",
    "    # Iterative refinement\n",
    "    for iteration in range(num_iterations):\n",
    "        # Compute average distance of each word to other words\n",
    "        avg_distances = []\n",
    "        for i in range(len(selected_words)):\n",
    "            word_i = selected_words[i]\n",
    "            distances = []\n",
    "            for j in range(len(selected_words)):\n",
    "                if i != j:\n",
    "                    dist = model.distance(word_i, selected_words[j])\n",
    "                    distances.append(dist)\n",
    "            avg_distance = np.mean(distances)\n",
    "            avg_distances.append(avg_distance)\n",
    "\n",
    "        # Identify the word with the lowest average distance\n",
    "        min_avg_dist_idx = np.argmin(avg_distances)\n",
    "        word_to_replace = selected_words[min_avg_dist_idx]\n",
    "        word_to_replace_idx = selected_indices[min_avg_dist_idx]\n",
    "\n",
    "        # Remove this word from the set\n",
    "        selected_words.pop(min_avg_dist_idx)\n",
    "        selected_indices.pop(min_avg_dist_idx)\n",
    "        selected_vecs.pop(min_avg_dist_idx)\n",
    "\n",
    "        # Add the index back to remaining indices\n",
    "        remaining_indices = np.append(remaining_indices, word_to_replace_idx)\n",
    "        remaining_indices.sort()\n",
    "\n",
    "        # Find a new word to replace it\n",
    "        # Compute distances between the removed word and remaining words\n",
    "        removed_vec = word_vectors[word_to_replace_idx].reshape(1, -1)\n",
    "        dists_to_removed = scipy.spatial.distance.cdist(word_vectors[remaining_indices], removed_vec, 'cosine').flatten()\n",
    "\n",
    "        # Also consider distances to current selected words\n",
    "        if selected_vecs:\n",
    "            dists_to_selected = scipy.spatial.distance.cdist(word_vectors[remaining_indices], np.vstack(selected_vecs), 'cosine')\n",
    "            min_dists_to_selected = np.min(dists_to_selected, axis=1)\n",
    "        else:\n",
    "            # If no selected words (unlikely), set min distances to zeros\n",
    "            min_dists_to_selected = np.zeros(len(remaining_indices))\n",
    "\n",
    "        # Combine distances (we can adjust weights if desired)\n",
    "        combined_scores = min_dists_to_selected + dists_to_removed\n",
    "\n",
    "        # Select the word with the maximum combined score\n",
    "        max_score_idx = np.argmax(combined_scores)\n",
    "        new_idx = remaining_indices[max_score_idx]\n",
    "\n",
    "        # Add the new word to the set\n",
    "        selected_indices.append(new_idx)\n",
    "        selected_words.append(words[new_idx])\n",
    "        selected_vecs.append(word_vectors[new_idx])\n",
    "\n",
    "        # Remove the new index from remaining indices\n",
    "        remaining_indices = np.delete(remaining_indices, max_score_idx)\n",
    "\n",
    "        # Recalculate the DAT score\n",
    "        new_score = model.dat(selected_words)\n",
    "        if new_score is not None and new_score > best_score:\n",
    "            best_score = new_score\n",
    "            best_words = selected_words.copy()\n",
    "        else:\n",
    "            # If no improvement, revert changes\n",
    "            # Remove the added word and restore the removed word\n",
    "            selected_indices.pop()\n",
    "            selected_words.pop()\n",
    "            selected_vecs.pop()\n",
    "            remaining_indices = np.append(remaining_indices, new_idx)\n",
    "            remaining_indices.sort()\n",
    "\n",
    "            selected_indices.insert(min_avg_dist_idx, word_to_replace_idx)\n",
    "            selected_words.insert(min_avg_dist_idx, word_to_replace)\n",
    "            selected_vecs.insert(min_avg_dist_idx, word_vectors[word_to_replace_idx])\n",
    "            remaining_indices = remaining_indices[remaining_indices != word_to_replace_idx]\n",
    "            remaining_indices.sort()\n",
    "\n",
    "    return best_words, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model=\"../../data/glove.840B.300d.txt\", dictionary=\"../../data/words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "\n",
    "tries = []\n",
    "\n",
    "for j in range(i):\n",
    "    print(f\"iteration: {j}\")\n",
    "    best_words, best_score = find_max_dissimilar_words(model, k=7, num_iterations=30)\n",
    "    tries.append((best_words, best_score))\n",
    "\n",
    "tries.sort(key=lambda tries: tries[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['tangency', 'dress', 'invention', 'interestingly', 'vac', 'tiff', 'dietitians']\n",
      "DAT Score: 105.75741213702021\n",
      "Words: ['plundering', 'rocky', 'interestingly', 'tiff', 'vac', 'sec', 'headrail']\n",
      "DAT Score: 105.5264950885127\n",
      "Words: ['his', 'raid', 'quote', 'antonyms', 'fir', 'xviii', 'hwy']\n",
      "DAT Score: 105.19561279299003\n",
      "Words: ['unspoken', 'gnome', 'sec', 'scuba', 'meted', 'coneflower', 'arm']\n",
      "DAT Score: 105.01957884989679\n",
      "Words: ['dandified', 'sec', 'blast', 'acer', 'forklifts', 'carisoprodol', 'culminated']\n",
      "DAT Score: 104.9819339722013\n",
      "Words: ['tails', 'batching', 'interestingly', 'sec', 'boulevards', 'arm', 'refoulement']\n",
      "DAT Score: 104.50428908031124\n",
      "Words: ['elysian', 'interestingly', 'sec', 'tachycardia', 'learn', 'cokes', 'acer']\n",
      "DAT Score: 104.36084837613937\n",
      "Words: ['patronisingly', 'date', 'jumper', 'acer', 'lactations', 'elf', 'plc']\n",
      "DAT Score: 104.04705697770365\n",
      "Words: ['timidity', 'jun', 'duty', 'arch-rivals', 'sac', 'analyzes', 'qwerty']\n",
      "DAT Score: 103.52325921412557\n",
      "Words: ['unpin', 'oral', 'ringleader', 'geo', 'dwindled', 'geomancy', 'fir']\n",
      "DAT Score: 103.14159769775524\n"
     ]
    }
   ],
   "source": [
    "for best_words, best_score in tries:\n",
    "    print(\"Words:\", best_words)\n",
    "    print(\"DAT Score:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
