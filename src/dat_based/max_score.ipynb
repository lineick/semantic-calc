{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import scipy\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def load_model_vectors(model_path, words_set):\n",
    "    vectors = {}\n",
    "    with open(model_path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.rstrip().split(\" \")\n",
    "            word = tokens[0].lower()\n",
    "            if word in words_set:\n",
    "                vector = np.asarray(tokens[1:], dtype=\"float32\")\n",
    "                vectors[word] = vector\n",
    "    return vectors\n",
    "\n",
    "\n",
    "# Update Model class to store words and vectors as NumPy arrays\n",
    "class Model:\n",
    "    def __init__(self, model=\"glove.840B.300d.txt\", dictionary=\"words.txt\", pattern=\"^[a-z][a-z-]*[a-z]$\"):\n",
    "        words_set = set()\n",
    "        with open(dictionary, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if re.match(pattern, line):\n",
    "                    words_set.add(line.lower())\n",
    "\n",
    "        self.vectors_dict = load_model_vectors(model, words_set)\n",
    "        self.words_array = np.array(list(self.vectors_dict.keys()))\n",
    "        self.word_vectors = np.array(list(self.vectors_dict.values()))\n",
    "        self.vectors = self.vectors_dict  # For compatibility\n",
    "\n",
    "    def validate(self, word):\n",
    "        \"\"\"Clean up word and find best candidate to use\"\"\"\n",
    "\n",
    "        # Strip unwanted characters\n",
    "        clean = re.sub(r\"[^a-zA-Z- ]+\", \"\", word).strip().lower()\n",
    "        if len(clean) <= 1:\n",
    "            return None  # Word too short\n",
    "\n",
    "        # Generate candidates for possible compound words\n",
    "        candidates = []\n",
    "        if \" \" in clean:\n",
    "            candidates.append(re.sub(r\" +\", \"-\", clean))\n",
    "            candidates.append(re.sub(r\" +\", \"\", clean))\n",
    "        else:\n",
    "            candidates.append(clean)\n",
    "            if \"-\" in clean:\n",
    "                candidates.append(re.sub(r\"-+\", \"\", clean))\n",
    "        for cand in candidates:\n",
    "            if cand in self.vectors:\n",
    "                return cand  # Return first word that is in model\n",
    "        return None  # Could not find valid word\n",
    "\n",
    "    def distance(self, word1, word2):\n",
    "        \"\"\"Compute cosine distance (0 to 2) between two words\"\"\"\n",
    "        return scipy.spatial.distance.cosine(self.vectors.get(word1), self.vectors.get(word2))\n",
    "\n",
    "    def dat(self, words, minimum=7):\n",
    "        \"\"\"Compute DAT score\"\"\"\n",
    "        # Keep only valid unique words\n",
    "        uniques = []\n",
    "        for word in words:\n",
    "            valid = self.validate(word)\n",
    "            if valid and valid not in uniques:\n",
    "                uniques.append(valid)\n",
    "\n",
    "        # Keep subset of words\n",
    "        if len(uniques) >= minimum:\n",
    "            subset = uniques[:minimum]\n",
    "        else:\n",
    "            return None  # Not enough valid words\n",
    "\n",
    "        # Compute distances between each pair of words\n",
    "        distances = []\n",
    "        for word1, word2 in itertools.combinations(subset, 2):\n",
    "            dist = self.distance(word1, word2)\n",
    "            distances.append(dist)\n",
    "\n",
    "        # Compute the DAT score (average semantic distance multiplied by 100)\n",
    "        return (sum(distances) / len(distances)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(args):\n",
    "    seed, words, word_vectors, k = args\n",
    "    np.random.seed(seed)\n",
    "    selected_indices = []\n",
    "    remaining_indices = np.arange(len(words))\n",
    "\n",
    "    # Randomly select the first word\n",
    "    first_idx = np.random.choice(remaining_indices)\n",
    "    selected_indices.append(first_idx)\n",
    "    remaining_indices = np.delete(remaining_indices, np.where(remaining_indices == first_idx))\n",
    "\n",
    "    selected_vecs = [word_vectors[first_idx]]\n",
    "\n",
    "    for _ in range(k - 1):\n",
    "        # Compute distances from selected words to all remaining words\n",
    "        dists = cdist(word_vectors[remaining_indices], np.vstack(selected_vecs), 'cosine')\n",
    "        min_dists = np.min(dists, axis=1)\n",
    "\n",
    "        # Select the word with the maximum of these minimum distances\n",
    "        max_min_dist_idx = np.argmax(min_dists)\n",
    "        next_idx = remaining_indices[max_min_dist_idx]\n",
    "        selected_indices.append(next_idx)\n",
    "        selected_vecs.append(word_vectors[next_idx])\n",
    "\n",
    "        # Remove selected index from remaining\n",
    "        remaining_indices = np.delete(remaining_indices, max_min_dist_idx)\n",
    "\n",
    "    selected_words = [words[idx] for idx in selected_indices]\n",
    "\n",
    "    # Compute DAT score directly\n",
    "    selected_vectors = np.vstack(selected_vecs)\n",
    "    pairwise_distances = cdist(selected_vectors, selected_vectors, 'cosine')\n",
    "\n",
    "    # Extract upper triangle without the diagonal\n",
    "    triu_indices = np.triu_indices(k, k=1)\n",
    "    distances = pairwise_distances[triu_indices]\n",
    "\n",
    "    # Compute the DAT score\n",
    "    score = np.mean(distances) * 100\n",
    "    return score, selected_words\n",
    "\n",
    "def find_max_dissimilar_words(model, k=7, num_restarts=10):\n",
    "    words = model.words_array\n",
    "    word_vectors = model.word_vectors\n",
    "\n",
    "    best_score = -1\n",
    "    best_words = None\n",
    "\n",
    "    seeds = [random.randint(0, int(1e8)) for _ in range(num_restarts)]\n",
    "    args_list = [(seed, words, word_vectors, k) for seed in seeds]\n",
    "\n",
    "    pool = multiprocessing.Pool()\n",
    "    results = pool.map(greedy_search, args_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    for score, selected_words in results:\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_words = selected_words\n",
    "\n",
    "    return best_words, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model=\"../../data/glove.840B.300d.txt\", dictionary=\"../../data/words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Words: ['ocelli', 'brand', 'geo', 'culminated', 'sec', 'herbalism', 'self-respecting']\n",
      "DAT Score: 106.73878455664895\n",
      "Time Taken: 292.5000169277191 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "selected_words, score = find_max_dissimilar_words(model, k=10, num_restarts=10)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Selected Words:\", selected_words)\n",
    "print(\"DAT Score:\", score)\n",
    "print(\"Time Taken:\", end_time - start_time, \"seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
